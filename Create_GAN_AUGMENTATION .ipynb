{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Create_GAN_AUGMENTATION.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FBd_DOi8IV9S"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzE0AokW94kX"
      },
      "source": [
        "#random\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ub9LGu96-r"
      },
      "source": [
        "seed = 666\n",
        "# python RNG\n",
        "import random\n",
        "random.seed(seed)\n",
        "# pytorch RNGs\n",
        "import torch\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "# numpy RNG\n",
        "import numpy as np\n",
        "np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBd_DOi8IV9S"
      },
      "source": [
        "#dataset 0 task2 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCk09WLDIU2I",
        "outputId": "e428fbe9-025f-4336-cb86-8f9792d8d486"
      },
      "source": [
        "!git clone https://github.com/zae-bayern/elpv-dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'elpv-dataset'...\n",
            "remote: Enumerating objects: 2686, done.\u001b[K\n",
            "remote: Total 2686 (delta 0), reused 0 (delta 0), pack-reused 2686\u001b[K\n",
            "Receiving objects: 100% (2686/2686), 90.79 MiB | 33.13 MiB/s, done.\n",
            "Resolving deltas: 100% (30/30), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3LHpC8JbtK0"
      },
      "source": [
        "dataset_id=0\n",
        "dataset_name=\"cells_eng\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUV3YssLMg-x"
      },
      "source": [
        "import os\n",
        "os.makedirs(\"/content/Task_Training_Input\"+str(dataset_id), exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Y9vzEN5lM_U7",
        "outputId": "69f4f319-9326-4027-b79b-176413ad62c8"
      },
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "df = pd.read_csv(\"/content/elpv-dataset/labels.csv\",delim_whitespace=True ,names=[\"image\",\"prob\",\"type\"])\n",
        "li=[]\n",
        "for i in list(df[\"image\"]):\n",
        "  word=i.split(\"/\")[1]\n",
        "  li.append(word.split(\".\")[0])\n",
        "df[\"image\"]=li\n",
        "#print(df)\n",
        "classes=[]\n",
        "for index, row in df.iterrows():\n",
        "  shutil.copy2(\"/content/elpv-dataset/images/\"+row[\"image\"]+\".png\", \"/content/Task_Training_Input\"+str(dataset_id)+\"/\")\n",
        "  c=0\n",
        "  if row[\"prob\"]== 0.0 :\n",
        "    c=0\n",
        "  elif row[\"prob\"]== 0.3333333333333333 :\n",
        "    c=1  \n",
        "  elif row[\"prob\"]== 0.6666666666666666 :  \n",
        "    c=2\n",
        "  elif row[\"prob\"]== 1.0 :      \n",
        "    c=3\n",
        "  #if row[\"type\"]== \"mono\":\n",
        "    #c+=4\n",
        "  classes.append(c)\n",
        "df[\"class\"]=classes\n",
        "df=df[[\"image\",\"class\"]]\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "df = shuffle(df,random_state=666)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>909</th>\n",
              "      <td>cell0910</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1591</th>\n",
              "      <td>cell1592</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>540</th>\n",
              "      <td>cell0541</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1747</th>\n",
              "      <td>cell1748</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2482</th>\n",
              "      <td>cell2483</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1993</th>\n",
              "      <td>cell1994</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>cell0071</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1950</th>\n",
              "      <td>cell1951</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1922</th>\n",
              "      <td>cell1923</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2284</th>\n",
              "      <td>cell2285</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2624 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         image  class\n",
              "909   cell0910      0\n",
              "1591  cell1592      0\n",
              "540   cell0541      0\n",
              "1747  cell1748      1\n",
              "2482  cell2483      1\n",
              "...        ...    ...\n",
              "1993  cell1994      0\n",
              "70    cell0071      0\n",
              "1950  cell1951      0\n",
              "1922  cell1923      2\n",
              "2284  cell2285      0\n",
              "\n",
              "[2624 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4zhT1nW2NHK",
        "outputId": "862b0a2b-c30e-48a3-c713-4c436a329113"
      },
      "source": [
        "li = list(df[\"class\"].values)\n",
        "my_dict = {i:li.count(i) for i in li}\n",
        "my_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 1508, 1: 295, 2: 106, 3: 715}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Loq5TKnwcCA5"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df_cell_all, test_df_cell_all = train_test_split(df, test_size=0.2,random_state=42, stratify= df[\"class\"])\n",
        "df_cell_all=pd.DataFrame(df)\n",
        "df_new=train_df_cell_all "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHzmfKiLv_M2"
      },
      "source": [
        "train_df, test_df = train_test_split(df_new, test_size=0.2,random_state=42, stratify= df_new[\"class\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcCAm9sjkSjf",
        "outputId": "b0b90d80-63ca-4719-dcc3-804a013086ab"
      },
      "source": [
        "my_dict[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1508"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOHfd7fRSQsq",
        "outputId": "6a3d633a-c272-47fb-f589-df8e04c56515"
      },
      "source": [
        "#f = lambda x: x-4 if x>3 else x\n",
        "#test_df_cell_all['class'] = test_df_cell_all['class'].map(f)\n",
        "#test_df['class'] = test_df['class'].map(f)\n",
        "#train_df['class'] = train_df['class'].map(f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1smf0x3xA5p",
        "outputId": "b23716f6-1a14-4a90-dfd4-2ecb6c051b24"
      },
      "source": [
        "li = list(train_df[\"class\"].values)\n",
        "my_dict = {i:li.count(i) for i in li}\n",
        "my_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 965, 1: 189, 2: 68, 3: 457}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkG5_y6FjQtU"
      },
      "source": [
        "#4 classses\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-7GFG7ySbMM"
      },
      "source": [
        "X_train_=[]\n",
        "Y_train_=[]\n",
        "def repeat_img(name,classes,repeat):\n",
        "  for i in range(repeat):\n",
        "    image = Image.open('/content/elpv-dataset/images/'+name+'.png') \n",
        "    image = image.resize((224,224))\n",
        "    numpydata = list(asarray(image)) \n",
        "    X_train_.append(numpydata)\n",
        "    Y_train_.append(classes)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW1keM9KwJ2u",
        "outputId": "356fa66f-a562-49aa-e152-a877cd0d3f78"
      },
      "source": [
        "from PIL import Image \n",
        "from numpy import asarray \n",
        "import numpy as np\n",
        "\n",
        "counter=0\n",
        "for index, row in train_df.iterrows():\n",
        "  rep=1\n",
        "  if row[\"class\"]==1:\n",
        "    rep=5\n",
        "  elif row[\"class\"]==2:\n",
        "    rep=12 \n",
        "  elif row[\"class\"]==3:\n",
        "    rep=2    \n",
        "  repeat_img(row[\"image\"],row[\"class\"],rep)\n",
        "  #image = Image.open('/content/elpv-dataset/images/'+row[\"image\"]+'.png') \n",
        "  #image = image.resize((224,224))\n",
        "  #numpydata = list(asarray(image)) \n",
        "  #X_train_.append(numpydata)\n",
        "  #Y_train_.append(row[\"class\"])\n",
        "  counter+=1\n",
        "  if counter %100==0:\n",
        "    print(counter)\n",
        "X_train_=np.array(X_train_)\n",
        "Y_train_=np.array(Y_train_)\n",
        "def load_data_():\n",
        "  return X_train_ ,Y_train_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFfydW6wUq_A",
        "outputId": "8b149194-887d-439b-cb84-11a0978b62d6"
      },
      "source": [
        "list(Y_train_).count(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "965"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG9QSpQ4y0Gy",
        "outputId": "36a57ea7-4902-4ea6-f097-318618b3ad7b"
      },
      "source": [
        "print((X_train_).shape)\n",
        "print((Y_train_).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3640, 224, 224)\n",
            "(3640,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hfQo9ypjct0"
      },
      "source": [
        "#8 classses\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAmpW8aRjct6"
      },
      "source": [
        "X_train_=[]\n",
        "Y_train_=[]\n",
        "def repeat_img(name,classes,repeat):\n",
        "  for i in range(repeat):\n",
        "    image = Image.open('/content/elpv-dataset/images/'+name+'.png') \n",
        "    image = image.resize((224,224))\n",
        "    numpydata = list(asarray(image)) \n",
        "    X_train_.append(numpydata)\n",
        "    Y_train_.append(classes)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8nuYXDkkquL"
      },
      "source": [
        "import numpy as np\n",
        "for index, row in train_df.iterrows():\n",
        "  rep=np.ceil(my_dict[0]/my_dict[row[\"class\"]])\n",
        "  print(rep,row[\"class\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e7rHeCDjcuA",
        "outputId": "6ee50a19-bf76-44ed-fd69-1d7ed3f78b78"
      },
      "source": [
        "from PIL import Image \n",
        "from numpy import asarray \n",
        "import numpy as np\n",
        "\n",
        "counter=0\n",
        "for index, row in train_df.iterrows():\n",
        "  rep=int(np.ceil(my_dict[0]/my_dict[row[\"class\"]]))\n",
        "\n",
        "  repeat_img(row[\"image\"],row[\"class\"],rep)\n",
        "  #image = Image.open('/content/elpv-dataset/images/'+row[\"image\"]+'.png') \n",
        "  #image = image.resize((224,224))\n",
        "  #numpydata = list(asarray(image)) \n",
        "  #X_train_.append(numpydata)\n",
        "  #Y_train_.append(row[\"class\"])\n",
        "  counter+=1\n",
        "  if counter %100==0:\n",
        "    print(counter)\n",
        "X_train_=np.array(X_train_)\n",
        "Y_train_=np.array(Y_train_)\n",
        "def load_data_():\n",
        "  return X_train_ ,Y_train_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp_n0WvTjcuI",
        "outputId": "83f5c1e0-9439-44c8-c7c7-9ddcb2d337d1"
      },
      "source": [
        "list(Y_train_).count(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "589"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcrydXe9jcuQ",
        "outputId": "5c0825f1-4f2a-4bdb-deed-3bd06f242d71"
      },
      "source": [
        "print((X_train_).shape)\n",
        "print((Y_train_).shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5216, 224, 224)\n",
            "(5216,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JohAwIrFjYxA"
      },
      "source": [
        "#shuffle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8_FN6I6NHfd"
      },
      "source": [
        "import sklearn\n",
        "X_train_, Y_train_ = sklearn.utils.shuffle(X_train_, Y_train_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjGoKuP_lqGu"
      },
      "source": [
        "#GAN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "776dlcR4l4uN",
        "outputId": "043dd122-c628-45aa-95a5-818631ae9798"
      },
      "source": [
        "# example of training an conditional gan on the fashion mnist dataset\n",
        "from numpy import expand_dims\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "#from keras.datasets.fashion_mnist import load_data\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Concatenate\n",
        "\n",
        "# define the standalone discriminator model\n",
        "def define_discriminator(in_shape=(224,224,1), n_classes=8):\n",
        "\t# label input\n",
        "\tin_label = Input(shape=(1,))\n",
        "\t# embedding for categorical input\n",
        "\tli = Embedding(n_classes, 50)(in_label)\n",
        "\t# scale up to image dimensions with linear activation\n",
        "\tn_nodes = in_shape[0] * in_shape[1]\n",
        "\tli = Dense(n_nodes)(li)\n",
        "\t# reshape to additional channel\n",
        "\tli = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
        "\t# image input\n",
        "\tin_image = Input(shape=in_shape)\n",
        "\t# concat label as a channel\n",
        "\tmerge = Concatenate()([in_image, li])\n",
        "\t# downsample\n",
        "\tfe = Conv2D(128, (3,3), strides=(2,2), padding='same')(merge)\n",
        "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
        "\t# downsample\n",
        "\tfe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
        "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
        "# downsample\n",
        "\tfe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
        "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
        "# downsample\n",
        "\tfe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
        "\tfe = LeakyReLU(alpha=0.2)(fe) \n",
        "\t# flatten feature maps\n",
        "\tfe = Flatten()(fe)\n",
        "\t# dropout\n",
        "\tfe = Dropout(0.4)(fe)\n",
        "\t# output\n",
        "\tout_layer = Dense(1, activation='sigmoid')(fe)\n",
        "\t# define model\n",
        "\tmodel = Model([in_image, in_label], out_layer)\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# define the standalone generator model\n",
        "def define_generator(latent_dim, n_classes=8):\n",
        "\t# label input\n",
        "\tin_=14\n",
        "\tin_label = Input(shape=(1,))\n",
        "\t# embedding for categorical input\n",
        "\tli = Embedding(n_classes, 50)(in_label)\n",
        "\t# linear multiplication\n",
        "\tn_nodes = in_ * in_\n",
        "\tli = Dense(n_nodes)(li)\n",
        "\t# reshape to additional channel\n",
        "\tli = Reshape((in_, in_, 1))(li)\n",
        "\t# image generator input\n",
        "\tin_lat = Input(shape=(latent_dim,))\n",
        "\t# foundation for 7x7 image\n",
        "\tn_nodes = 128 * in_ * in_\n",
        "\tgen = Dense(n_nodes)(in_lat)\n",
        "\tgen = LeakyReLU(alpha=0.2)(gen)\n",
        "\tgen = Reshape((in_, in_, 128))(gen)\n",
        "\t# merge image gen and label input\n",
        "\tmerge = Concatenate()([gen, li])\n",
        "\t# upsample to 14x14\n",
        "\tgen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n",
        "\tgen = LeakyReLU(alpha=0.2)(gen)\n",
        "\t# upsample to 28x28\n",
        "\tgen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "\tgen = LeakyReLU(alpha=0.2)(gen)\n",
        "# upsample to 28x28\n",
        "\tgen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "\tgen = LeakyReLU(alpha=0.2)(gen) \n",
        "# upsample to 28x28\n",
        "\tgen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "\tgen = LeakyReLU(alpha=0.2)(gen)  \n",
        "\t# output\n",
        "\tout_layer = Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n",
        "\t# define model\n",
        "\tmodel = Model([in_lat, in_label], out_layer)\n",
        "\treturn model\n",
        "\n",
        "# define the combined generator and discriminator model, for updating the generator\n",
        "def define_gan(g_model, d_model):\n",
        "\t# make weights in the discriminator not trainable\n",
        "\td_model.trainable = False\n",
        "\t# get noise and label inputs from generator model\n",
        "\tgen_noise, gen_label = g_model.input\n",
        "\t# get image output from the generator model\n",
        "\tgen_output = g_model.output;print(gen_output)\n",
        "\t# connect image output and label input from generator as inputs to discriminator\n",
        "  \n",
        "\tgan_output = d_model([gen_output, gen_label])\n",
        "\t# define gan model as taking noise and label and outputting a classification\n",
        "\tmodel = Model([gen_noise, gen_label], gan_output)\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\treturn model\n",
        "\n",
        "# load fashion mnist images\n",
        "def load_real_samples():\n",
        "\t# load dataset\n",
        "\t(trainX, trainy)= load_data_()\n",
        "\t# expand to 3d, e.g. add channels\n",
        "\tX = expand_dims(trainX, axis=-1)\n",
        "\t# convert from ints to floats\n",
        "\tX = X.astype('float32')\n",
        "\t# scale from [0,255] to [-1,1]\n",
        "\tX = (X - 127.5) / 127.5\n",
        "\treturn [X, trainy]\n",
        "\n",
        "# # select real samples\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "\t# split into images and labels\n",
        "\timages, labels = dataset\n",
        "\t# choose random instances\n",
        "\tix = randint(0, images.shape[0], n_samples)\n",
        "\t# select images and labels\n",
        "\tX, labels = images[ix], labels[ix]\n",
        "\t# generate class labels\n",
        "\ty = ones((n_samples, 1))\n",
        "\treturn [X, labels], y\n",
        "\n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples, n_classes=8):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tz_input = x_input.reshape(n_samples, latent_dim)\n",
        "\t# generate labels\n",
        "\tlabels = randint(0, n_classes, n_samples)\n",
        "\treturn [z_input, labels]\n",
        "\n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "\t# generate points in latent space\n",
        "\tz_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
        "\t# predict outputs\n",
        "\timages = generator.predict([z_input, labels_input])\n",
        "\t# create class labels\n",
        "\ty = zeros((n_samples, 1))\n",
        "\treturn [images, labels_input], y\n",
        "\n",
        "# train the generator and discriminator\n",
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=32):\n",
        "\tbat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "\tbest=999 \n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_epochs):\n",
        "\t\tloss=0\n",
        "\t\tloss_g=0\n",
        "\t\tloss_d=0\n",
        "\t\tnew_b=0\n",
        "\t\t# enumerate batches over the training set\n",
        "\t\tfor j in range(bat_per_epo):\n",
        "\t\t\t# get randomly selected 'real' samples\n",
        "\t\t\t[X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
        "\t\t\t# update discriminator model weights\n",
        "\t\t\td_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n",
        "\t\t\t# generate 'fake' examples\n",
        "\t\t\t[X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "\t\t\t# update discriminator model weights\n",
        "\t\t\td_loss2, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n",
        "\t\t\t# prepare points in latent space as input for the generator\n",
        "\t\t\t[z_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n",
        "\t\t\t# create inverted labels for the fake samples\n",
        "\t\t\ty_gan = ones((n_batch, 1))\n",
        "\t\t\t# update the generator via the discriminator's error\n",
        "\t\t\tg_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
        "\t\t\tloss_g += g_loss\n",
        "\t\t\tloss_d += (d_loss1+d_loss2)/2\n",
        "\t\t\tnew_b=loss_g+loss_d\n",
        "\t\t\t# summarize loss on this batch\n",
        "\t\tprint('>%d,%d, d1=%.3f, g=%.3f' %\n",
        "\t\t\t\t(i+1, bat_per_epo, loss_d, loss_g))\n",
        "\t\tif new_b<best:\n",
        "\t\t\tg_model.save('cgan_generator_'+str(new_b)+'_.h5')\n",
        "\t\t\tbest=new_b\n",
        "\t\t\tprint(\"saved\")\n",
        "def train1(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=32):\n",
        "\tbat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "\tbest=999 \n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_epochs):\n",
        "\t\tloss=0\n",
        "\t\tloss_g=0\n",
        "\t\tloss_d=0\n",
        "\t\tnew_b=0\n",
        "\t\tnew_acc1=0\n",
        "\t\tnew_acc2=0        \n",
        "\t\t# enumerate batches over the training set\n",
        "\t\tfor j in range(bat_per_epo):\n",
        "\t\t\t# get randomly selected 'real' samples\n",
        "\t\t\t[X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
        "\t\t\t# update discriminator model weights\n",
        "\t\t\td_loss1, acc1 = d_model.train_on_batch([X_real, labels_real], y_real)\n",
        "\t\t\t# generate 'fake' examples\n",
        "\t\t\t[X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "\t\t\t# update discriminator model weights\n",
        "\t\t\td_loss2, acc2 = d_model.train_on_batch([X_fake, labels], y_fake)\n",
        "\t\t\t# prepare points in latent space as input for the generator\n",
        "\t\t\t[z_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n",
        "\t\t\t# create inverted labels for the fake samples\n",
        "\t\t\ty_gan = ones((n_batch, 1))\n",
        "\t\t\t# update the generator via the discriminator's error\n",
        "\t\t\tg_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
        "\t\t\tloss_g += g_loss\n",
        "\t\t\tloss_d += (d_loss1+d_loss2)/2\n",
        "\t\t\tnew_b=loss_g+loss_d\n",
        "\t\t\tnew_acc1+=acc1\n",
        "\t\t\tnew_acc2+=acc2            \n",
        "\t\t\t# summarize loss on this batch\n",
        "\t\tprint('>%d,%d, d1=%.3f, g=%.3f' %\n",
        "\t\t\t\t(i+1, bat_per_epo, loss_d, loss_g))\n",
        "\t\tprint(\"real \"+str(new_acc1/bat_per_epo)+\" fake \"+str(new_acc2/bat_per_epo))  \n",
        "\t\tif new_b<best:\n",
        "\t\t\tg_model.save('cgan_generator_'+str(new_b)+'_.h5')\n",
        "\t\t\tbest=new_b\n",
        "\t\t\tprint(\"saved\")\n",
        "\t# save the generator model\n",
        "\tg_model.save('cgan_generator.h5')\n",
        "\n",
        "# size of the latent space\n",
        "latent_dim = 100\n",
        "# create the discriminator\n",
        "d_model = define_discriminator()\n",
        "# create the generator\n",
        "g_model = define_generator(latent_dim)\n",
        "# create the gan\n",
        "gan_model = define_gan(g_model, d_model)\n",
        "# load image data\n",
        "dataset = load_real_samples()\n",
        "# train model\n",
        "train1(g_model, d_model, gan_model, dataset, latent_dim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32, name=None), name='conv2d_9/Tanh:0', description=\"created by layer 'conv2d_9'\")\n",
            ">1,163, d1=48.001, g=559.976\n",
            "real 0.9121932515337423 fake 0.8711656441717791\n",
            "saved\n",
            ">2,163, d1=104.205, g=227.593\n",
            "real 0.6269171779141104 fake 0.6690950920245399\n",
            "saved\n",
            ">3,163, d1=112.375, g=138.168\n",
            "real 0.5026840490797546 fake 0.5621165644171779\n",
            "saved\n",
            ">4,163, d1=111.920, g=126.403\n",
            "real 0.4566717791411043 fake 0.602760736196319\n",
            "saved\n",
            ">5,163, d1=108.648, g=130.345\n",
            "real 0.5601993865030674 fake 0.6775306748466258\n",
            ">6,163, d1=101.682, g=150.343\n",
            "real 0.6062116564417178 fake 0.7296779141104295\n",
            ">7,163, d1=99.814, g=171.137\n",
            "real 0.6487730061349694 fake 0.6840490797546013\n",
            ">8,163, d1=106.401, g=145.189\n",
            "real 0.602760736196319 fake 0.6319018404907976\n",
            ">9,163, d1=107.038, g=142.177\n",
            "real 0.6062116564417178 fake 0.6196319018404908\n",
            ">10,163, d1=106.779, g=141.053\n",
            "real 0.6180981595092024 fake 0.620398773006135\n",
            ">11,163, d1=106.087, g=142.257\n",
            "real 0.5977760736196319 fake 0.6495398773006135\n",
            ">12,163, d1=105.027, g=146.602\n",
            "real 0.6062116564417178 fake 0.6568251533742331\n",
            ">13,163, d1=104.933, g=148.484\n",
            "real 0.6146472392638037 fake 0.6637269938650306\n",
            ">14,163, d1=105.059, g=148.017\n",
            "real 0.6085122699386503 fake 0.6575920245398773\n",
            ">15,163, d1=106.006, g=145.573\n",
            "real 0.598159509202454 fake 0.6522239263803681\n",
            ">16,163, d1=104.945, g=147.403\n",
            "real 0.616180981595092 fake 0.6572085889570553\n",
            ">17,163, d1=104.242, g=149.019\n",
            "real 0.6192484662576687 fake 0.6621932515337423\n",
            ">18,163, d1=104.844, g=149.151\n",
            "real 0.6023773006134969 fake 0.6805981595092024\n",
            ">19,163, d1=102.496, g=152.759\n",
            "real 0.6257668711656442 fake 0.6775306748466258\n",
            ">20,163, d1=101.680, g=157.672\n",
            "real 0.6330521472392638 fake 0.6744631901840491\n",
            ">21,163, d1=102.658, g=158.251\n",
            "real 0.6188650306748467 fake 0.6683282208588958\n",
            ">22,163, d1=101.945, g=158.381\n",
            "real 0.629217791411043 fake 0.6694785276073619\n",
            ">23,163, d1=101.253, g=162.116\n",
            "real 0.6368865030674846 fake 0.6721625766871165\n",
            ">24,163, d1=102.323, g=158.468\n",
            "real 0.6288343558282209 fake 0.6575920245398773\n",
            ">25,163, d1=102.122, g=159.692\n",
            "real 0.6142638036809815 fake 0.6690950920245399\n",
            ">26,163, d1=102.089, g=158.505\n",
            "real 0.6111963190184049 fake 0.6660276073619632\n",
            ">27,163, d1=101.420, g=159.699\n",
            "real 0.6299846625766872 fake 0.6748466257668712\n",
            ">28,163, d1=101.416, g=160.546\n",
            "real 0.6131134969325154 fake 0.6840490797546013\n",
            ">29,163, d1=101.773, g=158.945\n",
            "real 0.6138803680981595 fake 0.6840490797546013\n",
            ">30,163, d1=102.026, g=158.273\n",
            "real 0.6234662576687117 fake 0.6702453987730062\n",
            ">31,163, d1=101.606, g=157.414\n",
            "real 0.6180981595092024 fake 0.6702453987730062\n",
            ">32,163, d1=101.358, g=161.261\n",
            "real 0.6422546012269938 fake 0.6729294478527608\n",
            ">33,163, d1=100.066, g=162.942\n",
            "real 0.633819018404908 fake 0.6924846625766872\n",
            ">34,163, d1=99.187, g=167.602\n",
            "real 0.6303680981595092 fake 0.6940184049079755\n",
            ">35,163, d1=98.084, g=170.395\n",
            "real 0.6430214723926381 fake 0.7013036809815951\n",
            ">36,163, d1=97.260, g=172.331\n",
            "real 0.6449386503067485 fake 0.7124233128834356\n",
            ">37,163, d1=96.932, g=173.659\n",
            "real 0.6568251533742331 fake 0.7082055214723927\n",
            ">38,163, d1=96.468, g=177.469\n",
            "real 0.6591257668711656 fake 0.7108895705521472\n",
            ">39,163, d1=95.866, g=182.895\n",
            "real 0.6572085889570553 fake 0.7158742331288344\n",
            ">40,163, d1=97.139, g=179.661\n",
            "real 0.6483895705521472 fake 0.7039877300613497\n",
            ">41,163, d1=94.196, g=184.859\n",
            "real 0.6641104294478528 fake 0.7166411042944786\n",
            ">42,163, d1=93.271, g=187.740\n",
            "real 0.6721625766871165 fake 0.7254601226993865\n",
            ">43,163, d1=93.031, g=189.588\n",
            "real 0.6706288343558282 fake 0.7396472392638037\n",
            ">44,163, d1=93.139, g=189.116\n",
            "real 0.6725460122699386 fake 0.7212423312883436\n",
            ">45,163, d1=92.251, g=196.515\n",
            "real 0.6817484662576687 fake 0.7396472392638037\n",
            ">46,163, d1=91.197, g=195.493\n",
            "real 0.6901840490797546 fake 0.7358128834355828\n",
            ">47,163, d1=89.439, g=205.552\n",
            "real 0.700920245398773 fake 0.7580521472392638\n",
            ">48,163, d1=90.007, g=205.731\n",
            "real 0.6986196319018405 fake 0.745398773006135\n",
            ">49,163, d1=87.346, g=205.953\n",
            "real 0.7082055214723927 fake 0.75\n",
            ">50,163, d1=88.327, g=212.740\n",
            "real 0.6978527607361963 fake 0.74079754601227\n",
            ">51,163, d1=86.441, g=215.207\n",
            "real 0.7181748466257669 fake 0.7615030674846626\n",
            ">52,163, d1=86.829, g=224.809\n",
            "real 0.713957055214724 fake 0.7576687116564417\n",
            ">53,163, d1=84.419, g=226.843\n",
            "real 0.7085889570552147 fake 0.7703220858895705\n",
            ">54,163, d1=84.188, g=223.995\n",
            "real 0.7147239263803681 fake 0.7641871165644172\n",
            ">55,163, d1=84.217, g=228.146\n",
            "real 0.727760736196319 fake 0.7668711656441718\n",
            ">56,163, d1=82.597, g=236.291\n",
            "real 0.7335122699386503 fake 0.7718558282208589\n",
            ">57,163, d1=82.073, g=236.588\n",
            "real 0.7396472392638037 fake 0.7795245398773006\n",
            ">58,163, d1=79.734, g=244.320\n",
            "real 0.7461656441717791 fake 0.7917944785276073\n",
            ">59,163, d1=79.864, g=247.740\n",
            "real 0.745782208588957 fake 0.7906441717791411\n",
            ">60,163, d1=79.795, g=252.435\n",
            "real 0.7576687116564417 fake 0.7871932515337423\n",
            ">61,163, d1=80.342, g=251.944\n",
            "real 0.7369631901840491 fake 0.7783742331288344\n",
            ">62,163, d1=77.368, g=252.511\n",
            "real 0.7515337423312883 fake 0.7910276073619632\n",
            ">63,163, d1=77.109, g=266.618\n",
            "real 0.7603527607361963 fake 0.7875766871165644\n",
            ">64,163, d1=77.443, g=259.491\n",
            "real 0.7507668711656442 fake 0.7929447852760736\n",
            ">65,163, d1=75.998, g=269.280\n",
            "real 0.7638036809815951 fake 0.7952453987730062\n",
            ">66,163, d1=75.692, g=271.539\n",
            "real 0.7638036809815951 fake 0.7868098159509203\n",
            ">67,163, d1=73.244, g=277.124\n",
            "real 0.7707055214723927 fake 0.8105828220858896\n",
            ">68,163, d1=75.099, g=284.484\n",
            "real 0.7638036809815951 fake 0.7906441717791411\n",
            ">69,163, d1=73.255, g=284.236\n",
            "real 0.7718558282208589 fake 0.8017638036809815\n",
            ">70,163, d1=72.411, g=290.146\n",
            "real 0.7772239263803681 fake 0.8094325153374233\n",
            ">71,163, d1=71.601, g=288.318\n",
            "real 0.772239263803681 fake 0.8128834355828221\n",
            ">72,163, d1=71.564, g=305.891\n",
            "real 0.7772239263803681 fake 0.8071319018404908\n",
            ">73,163, d1=69.333, g=308.514\n",
            "real 0.7891104294478528 fake 0.8155674846625767\n",
            ">74,163, d1=68.949, g=313.936\n",
            "real 0.7864263803680982 fake 0.8125\n",
            ">75,163, d1=70.061, g=303.900\n",
            "real 0.7914110429447853 fake 0.817101226993865\n",
            ">76,163, d1=69.627, g=306.007\n",
            "real 0.7864263803680982 fake 0.8140337423312883\n",
            ">77,163, d1=68.613, g=313.364\n",
            "real 0.7975460122699386 fake 0.817101226993865\n",
            ">78,163, d1=67.088, g=312.501\n",
            "real 0.7948619631901841 fake 0.82170245398773\n",
            ">79,163, d1=68.299, g=328.374\n",
            "real 0.7894938650306749 fake 0.8182515337423313\n",
            ">80,163, d1=66.683, g=328.960\n",
            "real 0.8021472392638037 fake 0.8251533742331288\n",
            ">81,163, d1=65.896, g=332.229\n",
            "real 0.8101993865030674 fake 0.8332055214723927\n",
            ">82,163, d1=64.945, g=334.468\n",
            "real 0.7986963190184049 fake 0.8309049079754601\n",
            ">83,163, d1=63.084, g=347.040\n",
            "real 0.8029141104294478 fake 0.843558282208589\n",
            ">84,163, d1=64.577, g=343.314\n",
            "real 0.8098159509202454 fake 0.8282208588957055\n",
            ">85,163, d1=63.025, g=359.278\n",
            "real 0.8125 fake 0.8408742331288344\n",
            ">86,163, d1=63.239, g=360.584\n",
            "real 0.8144171779141104 fake 0.8324386503067485\n",
            ">87,163, d1=63.279, g=359.756\n",
            "real 0.8055981595092024 fake 0.8351226993865031\n",
            ">88,163, d1=62.236, g=357.074\n",
            "real 0.8125 fake 0.8412576687116564\n",
            ">89,163, d1=62.094, g=370.653\n",
            "real 0.8105828220858896 fake 0.8412576687116564\n",
            ">90,163, d1=61.372, g=375.372\n",
            "real 0.8190184049079755 fake 0.8324386503067485\n",
            ">91,163, d1=62.802, g=364.270\n",
            "real 0.8132668711656442 fake 0.8385736196319018\n",
            ">92,163, d1=60.245, g=375.417\n",
            "real 0.821319018404908 fake 0.8477760736196319\n",
            ">93,163, d1=60.403, g=384.105\n",
            "real 0.8197852760736196 fake 0.8362730061349694\n",
            ">94,163, d1=59.980, g=382.696\n",
            "real 0.8194018404907976 fake 0.8416411042944786\n",
            ">95,163, d1=57.257, g=389.606\n",
            "real 0.8374233128834356 fake 0.8569785276073619\n",
            ">96,163, d1=60.896, g=388.987\n",
            "real 0.8232361963190185 fake 0.8493098159509203\n",
            ">97,163, d1=58.628, g=397.425\n",
            "real 0.8263036809815951 fake 0.8477760736196319\n",
            ">98,163, d1=57.635, g=397.299\n",
            "real 0.8266871165644172 fake 0.8577453987730062\n",
            ">99,163, d1=57.816, g=403.609\n",
            "real 0.8301380368098159 fake 0.8554447852760736\n",
            ">100,163, d1=58.117, g=398.786\n",
            "real 0.8305214723926381 fake 0.8508435582822086\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "tIB6iFezKABd",
        "outputId": "c9ca1bcc-b019-4310-a011-f8f0ff97380e"
      },
      "source": [
        "# example of loading the generator model and generating images\n",
        "from numpy import asarray\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.models import load_model\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples, n_classes=4):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tz_input = x_input.reshape(n_samples, latent_dim)\n",
        "\t# generate labels\n",
        "\tlabels = randint(0, n_classes, n_samples)\n",
        "\treturn [z_input, labels]\n",
        "\n",
        "# create and save a plot of generated images\n",
        "def save_plot(examples, n,n2):\n",
        "\t# plot images\n",
        "\tfor i in range(n * n2):\n",
        "\t\t# define subplot\n",
        "\t\tpyplot.subplot(n, n2, 1 + i)\n",
        "\t\t# turn off axis\n",
        "\t\tpyplot.axis('off')\n",
        "\t\t# plot raw pixel data\n",
        "\t\tpyplot.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
        "\t#plt.rcParams['figure.figsize'] = [20, 20]  \n",
        "\tpyplot.show()\n",
        "\n",
        "# load model\n",
        "model = load_model('/content/cgan_generator.h5')\n",
        "# generate images\n",
        "latent_points, labels = generate_latent_points(100, 800)\n",
        "# specify labels\n",
        "labels = asarray([x for _ in range(100) for x in range(8)])\n",
        "print(labels)\n",
        "# generate images\n",
        "X  = model.predict([latent_points, labels])\n",
        "# scale from [-1,1] to [0,1]\n",
        "X = (X + 1) / 2.0\n",
        "# plot the result\n",
        "\n",
        "save_plot(X, 4,100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "[0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4\n",
            " 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1\n",
            " 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6\n",
            " 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3\n",
            " 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0\n",
            " 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5\n",
            " 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2\n",
            " 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7\n",
            " 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4\n",
            " 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1\n",
            " 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6\n",
            " 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3\n",
            " 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0\n",
            " 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5\n",
            " 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2\n",
            " 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7\n",
            " 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4\n",
            " 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1\n",
            " 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6\n",
            " 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3\n",
            " 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0\n",
            " 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 400 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1d2mbP7KYDH"
      },
      "source": [
        "X=(X*255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPvKpvqYzWlL"
      },
      "source": [
        "from PIL import Image#51,32\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "for i in range(X.shape[0]):\n",
        "  print(labels[i])\n",
        "  PIL_image = Image.fromarray(X[i].reshape(224,224))\n",
        "  PIL_image = PIL_image.convert(\"L\")\n",
        "  display(PIL_image)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsdkFPnDung9",
        "outputId": "9bedb29a-539a-4242-868c-214fd5293940"
      },
      "source": [
        "from math import floor\n",
        "from numpy import ones\n",
        "from numpy import expand_dims\n",
        "from numpy import log\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import exp\n",
        "from numpy.random import shuffle\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from keras.datasets import cifar10\n",
        "from skimage.transform import resize\n",
        "from numpy import asarray\n",
        " \n",
        "# scale an array of images to a new size\n",
        "def scale_images(images, new_shape):\n",
        "\tprint(images.shape)\n",
        "\timages=np.squeeze(images, axis=(3,))\n",
        "\timages=np.repeat(images[..., np.newaxis], 3, -1)\n",
        "\tprint(images.shape) \n",
        "\timages_list = list()\n",
        "\tfor image in images:\n",
        "\t\t# resize with nearest neighbor interpolation\n",
        "\t\tnew_image = resize(image, new_shape, 0)\n",
        "\t\t# store\n",
        "\t\timages_list.append(new_image)\n",
        "\treturn asarray(images_list)\n",
        " \n",
        "# assumes images have any shape and pixels in [0,255]\n",
        "def calculate_inception_score(images, n_split=10, eps=1E-16):\n",
        "\t# load inception v3 model\n",
        "\tmodel = InceptionV3()\n",
        "\t# enumerate splits of images/predictions\n",
        "\tscores = list()\n",
        "\tn_part = floor(images.shape[0] / n_split)\n",
        "\tfor i in range(n_split):\n",
        "\t\t# retrieve images\n",
        "\t\tix_start, ix_end = i * n_part, (i+1) * n_part\n",
        "\t\tsubset = images[ix_start:ix_end]\n",
        "\t\t# convert from uint8 to float32\n",
        "\t\tsubset = subset.astype('float32')\n",
        "\t\t# scale images to the required size\n",
        "\t\tsubset = scale_images(subset, (299,299,3))\n",
        "\t\t# pre-process images, scale to [-1,1]\n",
        "\t\tsubset = preprocess_input(subset)\n",
        "\t\t# predict p(y|x)\n",
        "\t\tp_yx = model.predict(subset)\n",
        "\t\t# calculate p(y)\n",
        "\t\tp_y = expand_dims(p_yx.mean(axis=0), 0)\n",
        "\t\t# calculate KL divergence using log probabilities\n",
        "\t\tkl_d = p_yx * (log(p_yx + eps) - log(p_y + eps))\n",
        "\t\t# sum over classes\n",
        "\t\tsum_kl_d = kl_d.sum(axis=1)\n",
        "\t\t# average over images\n",
        "\t\tavg_kl_d = mean(sum_kl_d)\n",
        "\t\t# undo the log\n",
        "\t\tis_score = exp(avg_kl_d)\n",
        "\t\t# store\n",
        "\t\tscores.append(is_score)\n",
        "\t# average across images\n",
        "\tis_avg, is_std = mean(scores), std(scores)\n",
        "\treturn is_avg, is_std\n",
        " \n",
        "# load cifar10 images\n",
        "images=X\n",
        "# shuffle images\n",
        "shuffle(images)\n",
        "print('loaded', images.shape)\n",
        "# calculate inception score\n",
        "is_avg, is_std = calculate_inception_score(images)\n",
        "print('score', is_avg, is_std)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded (400, 200, 200, 1)\n",
            "(40, 200, 200, 1)\n",
            "(40, 200, 200, 3)\n",
            "(40, 200, 200, 1)\n",
            "(40, 200, 200, 3)\n",
            "(40, 200, 200, 1)\n",
            "(40, 200, 200, 3)\n",
            "(40, 200, 200, 1)\n",
            "(40, 200, 200, 3)\n",
            "(40, 200, 200, 1)\n",
            "(40, 200, 200, 3)\n",
            "(40, 200, 200, 1)\n",
            "(40, 200, 200, 3)\n",
            "(40, 200, 200, 1)\n",
            "(40, 200, 200, 3)\n",
            "(40, 200, 200, 1)\n",
            "(40, 200, 200, 3)\n",
            "(40, 200, 200, 1)\n",
            "(40, 200, 200, 3)\n",
            "(40, 200, 200, 1)\n",
            "(40, 200, 200, 3)\n",
            "score 1.5369967 0.101828516\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}